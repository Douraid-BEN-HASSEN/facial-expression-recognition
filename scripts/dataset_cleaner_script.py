# Script for Dataset Cleaning
import cv2
from pathlib import Path
from typing import Tuple
from tqdm import tqdm
import sys
import os

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from classes.FaceLandmarkExtractor import FaceLandmarkExtractor

DATASET_PATH = "./datasets/fer2013_images"
THRESHOLD = 0.5 # Confidence threshold for face detection

def get_invalid_images_from_dataset(
    dataset_root: str,
    threshold: float = 0.5
) -> Tuple[str]:
    """
    Extrait les features de landmarks depuis un dataset organisÃ© en sous-dossiers par Ã©motion
    et retourne les images oÃ¹ aucun visage n'a Ã©tÃ© dÃ©tectÃ©.
    
    Structure attendue du dataset :
    dataset_root/
    â”œâ”€â”€ angry/
    â”‚   â”œâ”€â”€ image1.jpg
    â”‚   â”œâ”€â”€ image2.jpg
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ disgust/
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ fear/
    â”œâ”€â”€ happy/
    â”œâ”€â”€ neutral/
    â”œâ”€â”€ sad/
    â””â”€â”€ surprise/
    
    Args:
        dataset_root: Chemin vers le dossier racine contenant les sous-dossiers d'Ã©motions
    
    Returns:
        Tuple[str]: Chemins vers les images invalides
    """

    # VÃ©rifier que le dossier racine existe
    dataset_path = Path(dataset_root)
    if not dataset_path.exists():
        raise FileNotFoundError(f"âŒ Le dossier {dataset_root} n'existe pas")
    
    print("="*70)
    print("ğŸ­ EXTRACTION DES FEATURES - FER2013")
    print("="*70)
    print(f"ğŸ“ Dataset racine : {dataset_root}\n")
    
    # Collecter tous les fichiers images par Ã©motion
    all_image_paths = []
    
    # Extensions d'images supportÃ©es
    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}
    
    # Parcourir chaque dossier d'Ã©motion
    for folder_name in os.listdir(dataset_path):
        emotion_folder = dataset_path / folder_name
        
        # Trouver toutes les images dans ce dossier
        images = [
            img for img in emotion_folder.iterdir() 
            if img.is_file() and img.suffix.lower() in image_extensions
        ]
        
        for img_path in images:
            all_image_paths.append(img_path)
        
        print(f"âœ… {folder_name:12} : {len(images):5} images")
    
    total_images = len(all_image_paths)
    
    if total_images == 0:
        raise ValueError("âŒ Aucune image trouvÃ©e dans les sous-dossiers")
    
    print(f"\nğŸ“Š Total images      : {total_images}")
    
    # Initialiser l'extracteur de landmarks
    print("\nğŸ”„ Initialisation de MediaPipe Face Mesh...")
    extractor = FaceLandmarkExtractor(
        static_image_mode=True,
        max_num_faces=1,
        refine_landmarks=True,
        min_detection_confidence=threshold
    )
    
    # Extraire les features pour toutes les images
    print(f"\nğŸ”„ Extraction des features sur {total_images} images...\n")
    
    features_list = []
    valid_count = 0
    invalid_count = 0
    invalid_paths = []
    
    for img_path in tqdm(all_image_paths, desc="Extraction", ncols=100):
        try:
            # Charger l'image
            image = cv2.imread(str(img_path))
            
            if image is None:
                print(f"âš ï¸  Impossible de charger : {img_path}")
                features_list.append(None)
                invalid_count += 1
                invalid_paths.append(str(img_path))
                continue
            
            # Extraire les landmarks
            landmarks = extractor.extract_landmarks(image)
            
            if landmarks is not None:
                # Extraire les features
                features = extractor.landmarks_to_features(landmarks)
                features_list.append(features)
                valid_count += 1
            else:
                features_list.append(None)
                invalid_count += 1
                invalid_paths.append(str(img_path))
        
        except Exception as e:
            print(f"âŒ Erreur sur {img_path}: {e}")
            features_list.append(None)
            invalid_count += 1
            invalid_paths.append(str(img_path))
    
    # Fermer MediaPipe proprement
    del extractor
    
    # Statistiques
    valid_rate = (valid_count / total_images) * 100
    
    print("\n" + "="*70)
    print("ğŸ“Š RÃ‰SULTATS DE L'EXTRACTION")
    print("="*70)
    print(f"âœ… Visages dÃ©tectÃ©s      : {valid_count} ({valid_rate:.2f}%)")
    print(f"âŒ Visages non dÃ©tectÃ©s  : {invalid_count} ({100-valid_rate:.2f}%)")
    
    return invalid_paths

def delete_invalid_images(invalid_image_paths: Tuple[str]):
    """
    Supprime les images invalides du dataset.
    
    Args:
        invalid_image_paths: Chemins vers les images invalides
    """
    print("\nğŸ—‘ï¸  Suppression des images invalides...")
    for img_path in invalid_image_paths:
        try:
            os.remove(img_path)
            print(f"âœ… SupprimÃ© : {img_path}")
        except Exception as e:
            print(f"âŒ Erreur lors de la suppression de {img_path}: {e}")

# check if DATASET_PATH exists
if not os.path.exists(DATASET_PATH):
    raise FileNotFoundError(f"The specified dataset path '{DATASET_PATH}' does not exist.")

if __name__ == "__main__":
    invalid_paths = get_invalid_images_from_dataset(
        dataset_root=DATASET_PATH,
        threshold=THRESHOLD
    )
    
    # Delete invalid images
    delete_invalid_images(invalid_image_paths=invalid_paths)

    print(f"âœ… Cleaning done. {len(invalid_paths)} images removed.")

