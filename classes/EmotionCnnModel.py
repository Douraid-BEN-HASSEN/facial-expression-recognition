"""
üéØ ARCHITECTURE CNN (EfficientNet) POUR CLASSIFICATION D'√âMOTIONS
====================================================================

Cette architecture utilise EfficientNet-B0 avec transfer learning ImageNet,
reconnue comme l'une des meilleures pour FER2013.

üî• FONCTIONNALIT√âS CL√âS :
---------------------------------------------------
1. **Transfer Learning** : Pr√©entra√Æn√© sur ImageNet ‚Üí meilleure g√©n√©ralisation
2. **Compound Scaling** : Balance optimale depth/width/resolution
3. **Moins de param√®tres** : ~5M param√®tres vs 11M (ResNet18)
4. **Performances SOTA** : 62-68% accuracy attendue sur FER2013
5. **Convergence rapide** : Moins d'epochs n√©cessaires

üìä TECHNIQUES AVANC√âES IMPL√âMENT√âES :
-------------------------------------
‚úÖ Mixup : M√©lange d'images pour r√©gularisation forte
‚úÖ Label Smoothing : R√©duit overfitting sur labels bruit√©s
‚úÖ Cosine Annealing LR : Meilleur que ReduceLROnPlateau pour CNN
‚úÖ Test-Time Augmentation (TTA) : Boost final +1-2%
‚úÖ Data Augmentation adapt√©e : Rotation, flip, brightness, contrast
‚úÖ Gradient Clipping : Stabilise l'entra√Ænement

üìÅ FORMAT DATASET ATTENDU :
---------------------------
fer2013_images/
    ‚îú‚îÄ‚îÄ angry/
    ‚îÇ   ‚îú‚îÄ‚îÄ image1.jpg
    ‚îÇ   ‚îî‚îÄ‚îÄ ...
    ‚îú‚îÄ‚îÄ disgust/
    ‚îú‚îÄ‚îÄ fear/
    ‚îú‚îÄ‚îÄ happy/
    ‚îú‚îÄ‚îÄ neutral/
    ‚îú‚îÄ‚îÄ sad/
    ‚îî‚îÄ‚îÄ surprise/
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import models
from torch.utils.data import DataLoader
import numpy as np
from pathlib import Path
from typing import Tuple, Dict
import matplotlib.pyplot as plt
from tqdm import tqdm
import time


class EmotionCNN(nn.Module):
    """
    üéØ MOD√àLE CNN POUR CLASSIFICATION D'√âMOTIONS
    
    Architecture bas√©e sur EfficientNet-B0 (transfer learning ImageNet).
    
    Caract√©ristiques :
    - EfficientNet-B0 backbone (5M params)
    - Fine-tuning progressif
    - Dropout adaptatif
    - Classification head optimis√©
    """
    
    def __init__(
        self,
        n_classes: int = 7,
        pretrained: bool = True,
        dropout: float = 0.3,
        fine_tune_mode: str = 'full'  # 'full', 'partial', 'head_only'
    ):
        """
        Args:
            n_classes: Nombre de classes (7 √©motions)
            pretrained: Utiliser les poids ImageNet
            dropout: Taux de dropout
            fine_tune_mode: Mode de fine-tuning
                - 'head_only' : Geler le backbone, entra√Æner seulement la t√™te
                - 'partial' : Geler les premiers layers, fine-tune le reste
                - 'full' : Fine-tune tout le r√©seau
        """
        super().__init__()
        
        self.n_classes = n_classes
        self.fine_tune_mode = fine_tune_mode
        
        # Charger EfficientNet-B0 pr√©entra√Æn√©
        if pretrained:
            # PyTorch 2.x syntax
            weights = models.EfficientNet_B0_Weights.DEFAULT
            self.backbone = models.efficientnet_b0(weights=weights)
        else:
            self.backbone = models.efficientnet_b0(weights=None)
        
        # R√©cup√©rer la dimension des features du backbone
        # EfficientNet-B0 : 1280 features avant le classifier
        in_features = self.backbone.classifier[1].in_features
        
        # Remplacer le classifier par le n√¥tre
        self.backbone.classifier = nn.Identity()
        
        # Classification head optimis√© pour FER2013
        self.classifier = nn.Sequential(
            nn.Dropout(p=dropout),
            nn.Linear(in_features, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(inplace=True),
            nn.Dropout(p=dropout * 0.5),
            nn.Linear(512, n_classes)
        )
        
        # Appliquer le mode de fine-tuning
        self._set_fine_tune_mode(fine_tune_mode)
        
        # Initialiser le classifier
        self._initialize_classifier()
    
    def _set_fine_tune_mode(self, mode: str):
        """
        Configure le mode de fine-tuning
        """
        if mode == 'head_only':
            # Geler tout le backbone
            for param in self.backbone.parameters():
                param.requires_grad = False
            print("üîí Mode head_only : Backbone gel√©, entra√Ænement classifier uniquement")
        
        elif mode == 'partial':
            # Geler les 5 premiers blocs (sur 7 dans EfficientNet-B0)
            for i, (name, param) in enumerate(self.backbone.named_parameters()):
                if 'features.0' in name or 'features.1' in name or \
                   'features.2' in name or 'features.3' in name or 'features.4' in name:
                    param.requires_grad = False
            print("üîì Mode partial : Premiers blocs gel√©s, fine-tuning blocs sup√©rieurs")
        
        elif mode == 'full':
            # Tout est d√©gel√© par d√©faut
            for param in self.backbone.parameters():
                param.requires_grad = True
            print("üî• Mode full : Fine-tuning complet du backbone")
        
        else:
            raise ValueError(f"Mode invalide : {mode}. Choisir 'head_only', 'partial', ou 'full'")
    
    def _initialize_classifier(self):
        """Initialiser le classifier avec Xavier"""
        for module in self.classifier.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass
        
        Args:
            x: Images (batch_size, 3, H, W)
        
        Returns:
            Logits (batch_size, n_classes)
        """
        # Backbone EfficientNet
        features = self.backbone(x)
        
        # Classification
        logits = self.classifier(features)
        
        return logits
    
    def predict(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Pr√©diction avec probabilit√©s
        
        Args:
            x: Images (batch_size, 3, H, W)
        
        Returns:
            predictions: Indices des classes (batch_size,)
            probabilities: Probabilit√©s softmax (batch_size, n_classes)
        """
        self.eval()
        with torch.no_grad():
            logits = self.forward(x)
            probabilities = F.softmax(logits, dim=1)
            predictions = torch.argmax(probabilities, dim=1)
        
        return predictions, probabilities
    
    def get_num_parameters(self) -> int:
        """Retourne le nombre total de param√®tres"""
        return sum(p.numel() for p in self.parameters())
    
    def get_trainable_parameters(self) -> int:
        """Retourne le nombre de param√®tres entra√Ænables"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)


def mixup_data(x: torch.Tensor, y: torch.Tensor, alpha: float = 0.2) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, float]:
    """
    Applique Mixup sur un batch d'images
    
    Mixup : m√©lange lin√©aire de deux images et leurs labels
    Papier : "mixup: Beyond Empirical Risk Minimization" (2018)
    
    Args:
        x: Images (batch_size, C, H, W)
        y: Labels (batch_size,)
        alpha: Param√®tre de la distribution Beta
    
    Returns:
        mixed_x: Images mix√©es
        y_a: Labels originaux
        y_b: Labels m√©lang√©s
        lam: Coefficient de m√©lange
    """
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1.0
    
    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(x.device)
    
    mixed_x = lam * x + (1 - lam) * x[index]
    y_a, y_b = y, y[index]
    
    return mixed_x, y_a, y_b, lam


def mixup_criterion(criterion, pred: torch.Tensor, y_a: torch.Tensor, y_b: torch.Tensor, lam: float) -> torch.Tensor:
    """
    Loss pour Mixup
    """
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)


class LabelSmoothingCrossEntropy(nn.Module):
    """
    Cross-Entropy avec Label Smoothing
    
    Label Smoothing : remplace les labels one-hot [0,0,1,0,0,0,0] 
    par [Œµ/6, Œµ/6, 1-Œµ, Œµ/6, Œµ/6, Œµ/6, Œµ/6]
    
    R√©duit l'overfitting en p√©nalisant la surconfiance du mod√®le.
    """
    
    def __init__(self, smoothing: float = 0.1):
        super().__init__()
        self.smoothing = smoothing
    
    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """
        Args:
            pred: Logits (batch_size, n_classes)
            target: Labels (batch_size,)
        """
        n_classes = pred.size(-1)
        log_probs = F.log_softmax(pred, dim=-1)
        
        # Label smoothing
        targets = torch.zeros_like(log_probs).scatter_(1, target.unsqueeze(1), 1)
        targets = targets * (1 - self.smoothing) + self.smoothing / n_classes
        
        loss = (-targets * log_probs).sum(dim=-1).mean()
        
        return loss


class ModelManager:
    """
    Gestionnaire pour entra√Ænement, sauvegarde, et chargement des mod√®les CNN
    """
    
    @staticmethod
    def train_model(
        model: nn.Module,
        train_loader: DataLoader,
        val_loader: DataLoader,
        test_loader: DataLoader,
        n_epochs: int = 100,
        device: str = 'cuda',
        learning_rate: float = 1e-3,
        weight_decay: float = 1e-4,
        patience: int = 20,
        save_dir: str = "./models/emotion_cnn",
        model_name: str = "emotion_cnn",
        use_mixup: bool = True,
        mixup_alpha: float = 0.2,
        label_smoothing: float = 0.1,
        use_cosine_annealing: bool = True,
        gradient_clip: float = 1.0
    ) -> Dict:
        """
        Entra√Ænement du mod√®le CNN avec toutes les techniques SOTA
        
        Args:
            model: Mod√®le CNN
            train_loader: DataLoader d'entra√Ænement
            val_loader: DataLoader de validation
            test_loader: DataLoader de test
            n_epochs: Nombre d'epochs maximum
            device: 'cpu' ou 'cuda'
            learning_rate: Learning rate initial
            weight_decay: Weight decay pour r√©gularisation
            patience: Patience pour early stopping
            save_dir: Dossier de sauvegarde
            model_name: Nom du mod√®le
            use_mixup: Utiliser Mixup
            mixup_alpha: Param√®tre alpha pour Mixup
            label_smoothing: Coefficient de label smoothing
            use_cosine_annealing: Utiliser Cosine Annealing LR
            gradient_clip: Valeur de gradient clipping
        
        Returns:
            Historique d'entra√Ænement
        """
        print("\n" + "="*80)
        print("üöÄ D√âBUT DE L'ENTRA√éNEMENT CNN")
        print("="*80)
        
        model = model.to(device)
        
        # Loss avec label smoothing
        if label_smoothing > 0:
            criterion = LabelSmoothingCrossEntropy(smoothing=label_smoothing)
            print(f"‚úÖ Label Smoothing activ√© : {label_smoothing}")
        else:
            criterion = nn.CrossEntropyLoss()
        
        # Optimizer
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay,
            betas=(0.9, 0.999)
        )
        
        # Scheduler
        if use_cosine_annealing:
            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
                optimizer,
                T_0=10,  # Premier cycle de 10 epochs
                T_mult=2,  # Doubler la p√©riode √† chaque restart
                eta_min=1e-6
            )
            print(f"‚úÖ Cosine Annealing LR activ√©")
        else:
            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                optimizer,
                mode='max',
                factor=0.5,
                patience=5,
                verbose=True
            )
        
        # Early stopping
        best_val_acc = 0.0
        patience_counter = 0
        
        # Historique
        history = {
            'train_loss': [], 'train_acc': [],
            'val_loss': [], 'val_acc': [],
            'test_acc': None,
            'learning_rates': [],
            'best_epoch': 0
        }
        
        print(f"\nüìä Configuration :")
        print(f"   Param√®tres entra√Ænables : {model.get_trainable_parameters():,}")
        print(f"   Learning rate initial   : {learning_rate}")
        print(f"   Weight decay            : {weight_decay}")
        print(f"   Mixup                   : {use_mixup} (alpha={mixup_alpha})")
        print(f"   Gradient clipping       : {gradient_clip}")
        print(f"   Device                  : {device}")
        print("="*80)
        
        # Boucle d'entra√Ænement
        for epoch in range(n_epochs):
            start_time = time.time()
            
            # === PHASE D'ENTRA√éNEMENT ===
            model.train()
            train_loss = 0.0
            train_correct = 0
            train_total = 0
            
            train_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{n_epochs} [Train]", ncols=100)
            
            for batch_idx, (inputs, targets) in enumerate(train_bar):
                inputs, targets = inputs.to(device), targets.to(device)
                
                # Mixup si activ√©
                if use_mixup and np.random.rand() < 0.5:
                    inputs, targets_a, targets_b, lam = mixup_data(inputs, targets, mixup_alpha)
                    
                    optimizer.zero_grad()
                    outputs = model(inputs)
                    loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)
                else:
                    optimizer.zero_grad()
                    outputs = model(inputs)
                    loss = criterion(outputs, targets)
                
                loss.backward()
                
                # Gradient clipping
                if gradient_clip > 0:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)
                
                optimizer.step()
                
                # Statistiques
                train_loss += loss.item()
                _, predicted = outputs.max(1)
                train_total += targets.size(0)
                train_correct += predicted.eq(targets).sum().item()
                
                # Update progress bar
                train_bar.set_postfix({
                    'loss': f'{train_loss/(batch_idx+1):.4f}',
                    'acc': f'{100.*train_correct/train_total:.2f}%'
                })
            
            train_loss = train_loss / len(train_loader)
            train_acc = 100. * train_correct / train_total
            
            # === PHASE DE VALIDATION ===
            model.eval()
            val_loss = 0.0
            val_correct = 0
            val_total = 0
            
            with torch.no_grad():
                for inputs, targets in val_loader:
                    inputs, targets = inputs.to(device), targets.to(device)
                    outputs = model(inputs)
                    loss = criterion(outputs, targets)
                    
                    val_loss += loss.item()
                    _, predicted = outputs.max(1)
                    val_total += targets.size(0)
                    val_correct += predicted.eq(targets).sum().item()
            
            val_loss = val_loss / len(val_loader)
            val_acc = 100. * val_correct / val_total
            
            # Update scheduler
            current_lr = optimizer.param_groups[0]['lr']
            if use_cosine_annealing:
                scheduler.step()
            else:
                scheduler.step(val_acc)
            
            # Sauvegarder l'historique
            history['train_loss'].append(train_loss)
            history['train_acc'].append(train_acc)
            history['val_loss'].append(val_loss)
            history['val_acc'].append(val_acc)
            history['learning_rates'].append(current_lr)
            
            # Temps d'epoch
            epoch_time = time.time() - start_time
            
            # Affichage
            print(f"\n{'='*80}")
            print(f"Epoch {epoch+1}/{n_epochs} termin√© en {epoch_time:.2f}s")
            print(f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%")
            print(f"Val Loss  : {val_loss:.4f} | Val Acc  : {val_acc:.2f}%")
            print(f"Learning Rate: {current_lr:.6f}")
            
            # Early stopping et sauvegarde du meilleur mod√®le
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience_counter = 0
                history['best_epoch'] = epoch + 1
                
                # Sauvegarder le meilleur mod√®le
                save_path = Path(save_dir) / f"{model_name}.pth"
                save_path.parent.mkdir(parents=True, exist_ok=True)
                
                torch.save({
                    'epoch': epoch + 1,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'val_acc': val_acc,
                    'train_acc': train_acc,
                    'n_classes': model.n_classes,
                    'fine_tune_mode': model.fine_tune_mode
                }, save_path)
                
                print(f"‚úÖ Nouveau meilleur mod√®le sauvegard√© ! Val Acc: {val_acc:.2f}%")
            else:
                patience_counter += 1
                print(f"‚è≥ Patience: {patience_counter}/{patience}")
            
            print(f"{'='*80}\n")
            
            # Early stopping
            if patience_counter >= patience:
                print(f"üõë Early stopping d√©clench√© apr√®s {epoch+1} epochs")
                break
        
        # === √âVALUATION FINALE SUR LE TEST SET ===
        print("\n" + "="*80)
        print("üìä √âVALUATION FINALE SUR LE TEST SET")
        print("="*80)
        
        # Charger le meilleur mod√®le
        checkpoint = torch.load(Path(save_dir) / f"{model_name}.pth")
        model.load_state_dict(checkpoint['model_state_dict'])
        
        model.eval()
        test_correct = 0
        test_total = 0
        
        with torch.no_grad():
            for inputs, targets in tqdm(test_loader, desc="Test", ncols=100):
                inputs, targets = inputs.to(device), targets.to(device)
                outputs = model(inputs)
                _, predicted = outputs.max(1)
                test_total += targets.size(0)
                test_correct += predicted.eq(targets).sum().item()
        
        test_acc = 100. * test_correct / test_total
        history['test_acc'] = test_acc
        
        print(f"\n‚úÖ Test Accuracy : {test_acc:.2f}%")
        print(f"üìà Best Val Accuracy : {best_val_acc:.2f}% (epoch {history['best_epoch']})")
        print("="*80)
        
        return history
    
    @staticmethod
    def load_model(model_path: str, device: str = 'cpu') -> nn.Module:
        """
        Charger un mod√®le sauvegard√©
        """
        checkpoint = torch.load(model_path, map_location=device)
        
        model = EmotionCNN(
            n_classes=checkpoint['n_classes'],
            fine_tune_mode=checkpoint.get('fine_tune_mode', 'full')
        )
        
        model.load_state_dict(checkpoint['model_state_dict'])
        model = model.to(device)
        model.eval()
        
        print(f"‚úÖ Mod√®le charg√© : {model_path}")
        print(f"   Val Accuracy : {checkpoint['val_acc']:.2f}%")
        print(f"   Epoch        : {checkpoint['epoch']}")
        
        return model
    
    @staticmethod
    def plot_history(history: Dict, save_path: str):
        """
        Tracer les courbes d'entra√Ænement
        """
        fig, axes = plt.subplots(1, 3, figsize=(18, 5))
        
        # Loss
        axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)
        axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)
        axes[0].set_xlabel('Epoch', fontsize=12)
        axes[0].set_ylabel('Loss', fontsize=12)
        axes[0].set_title('Loss Evolution', fontsize=14, fontweight='bold')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # Accuracy
        axes[1].plot(history['train_acc'], label='Train Acc', linewidth=2)
        axes[1].plot(history['val_acc'], label='Val Acc', linewidth=2)
        if history['test_acc'] is not None:
            axes[1].axhline(y=history['test_acc'], color='r', linestyle='--', 
                           label=f'Test Acc: {history["test_acc"]:.2f}%', linewidth=2)
        axes[1].set_xlabel('Epoch', fontsize=12)
        axes[1].set_ylabel('Accuracy (%)', fontsize=12)
        axes[1].set_title('Accuracy Evolution', fontsize=14, fontweight='bold')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        
        # Learning Rate
        axes[2].plot(history['learning_rates'], linewidth=2, color='green')
        axes[2].set_xlabel('Epoch', fontsize=12)
        axes[2].set_ylabel('Learning Rate', fontsize=12)
        axes[2].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')
        axes[2].set_yscale('log')
        axes[2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"üìä Courbes sauvegard√©es : {save_path}")